# kronos_api.py
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, Depends
from pydantic import BaseModel, Field
from typing import List, Optional, Dict
import pandas as pd
import numpy as np
import torch
import logging
import os
from dotenv import load_dotenv

from model import Kronos, KronosTokenizer, KronosPredictor


# ================================================================
# 2. æ—¥å¿—é…ç½®
# ================================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    filename="logs/kronos_api.log",  # ä¿å­˜è·¯å¾„
    filemode="a"  # a=è¿½åŠ , w=è¦†ç›–
)
logger = logging.getLogger("kronos_api")

# ================================================================
# 3. å®šä¹‰è¯·æ±‚/å“åº”æ¨¡å‹
# ================================================================
class KlineData(BaseModel):
    open: float
    high: float
    low: float
    close: float
    volume: float
    amount: Optional[float] = None

class PredictRequest(BaseModel):
    kline_data: List[KlineData] = Field(..., description="å†å² K çº¿æ•°æ®ï¼Œå»ºè®®ä¸å°‘äº 10 æ¡")
    pred_len: int = Field(default=10, ge=1, le=200, description="é¢„æµ‹æ­¥é•¿ï¼ˆæœªæ¥é¢„æµ‹é•¿åº¦ï¼‰")
    freq: str = Field(default="1h", description="æ—¶é—´é—´éš”ï¼Œå¦‚ '1h', '5min', '1d'")
    T: float = Field(default=1.0, ge=0.1, le=2.0)
    top_p: float = Field(default=0.9, ge=0.5, le=1.0)
    sample_count: int = Field(default=1, ge=1, le=5)

class PredictResponse(BaseModel):
    code: int
    message: str
    data: Dict[str, Dict[str, float]]

# ================================================================
# 4. æ¨¡å‹åŠ è½½ï¼ˆå…¨å±€å•ä¾‹ï¼‰
# ================================================================

current_file_path = os.path.abspath(__file__)
project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_file_path)))
env_path = os.path.join(project_root, ".env")
load_dotenv(dotenv_path=env_path)
MODEL_PATH = os.environ.get("KRONOS_MODEL_PATH")
TOKENIZER_PATH = os.environ.get("KRONOS_TOKENIZER_PATH")
device = "cuda" if torch.cuda.is_available() else "cpu"

model = None
tokenizer = None
predictor = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    global model, tokenizer, predictor
    try:
        logger.info("ğŸš€ å¯åŠ¨æœåŠ¡ï¼ŒåŠ è½½æ¨¡å‹ä¸­...")
        tokenizer = KronosTokenizer.from_pretrained(TOKENIZER_PATH)
        model = Kronos.from_pretrained(MODEL_PATH).to(device)
        predictor = KronosPredictor(model, tokenizer, device=device)
        logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}")
        yield  # è¿™é‡Œè¿›å…¥åº”ç”¨æ­£å¸¸è¿è¡Œé˜¶æ®µ
    finally:
        logger.info("ğŸ›‘ æœåŠ¡å…³é—­ï¼Œé‡Šæ”¾èµ„æº...")
        # å¯ä»¥åœ¨æ­¤é‡Šæ”¾ GPU å†…å­˜
        del predictor, model, tokenizer
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        logger.info("âœ… èµ„æºé‡Šæ”¾å®Œæˆ")



app = FastAPI(
    title="Kronos é‡åŒ–é¢„æµ‹ API",
    description="åŸºäº Kronos æ¨¡å‹çš„ K çº¿æ—¶é—´åºåˆ—é¢„æµ‹æœåŠ¡ã€‚",
    version="1.1.0",
    lifespan=lifespan
)


# ================================================================
# 5. å·¥å…·å‡½æ•°
# ================================================================
def parse_timedelta(freq: str) -> pd.Timedelta:
    """å®‰å…¨è§£ææ—¶é—´é—´éš”å­—ç¬¦ä¸²ï¼Œå¦‚ 1h, 5min, 1dã€‚"""
    try:
        return pd.Timedelta(freq)
    except Exception:
        raise HTTPException(status_code=400, detail=f"æ— æ•ˆæ—¶é—´é¢‘ç‡å‚æ•°ï¼š{freq}")


def get_predictor() -> KronosPredictor:
    """ä¾èµ–æ³¨å…¥ï¼Œç¡®ä¿ predictor å·²åŠ è½½ã€‚"""
    if predictor is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½å®Œæˆ")
    return predictor


# ================================================================
# 6. ä¸»é¢„æµ‹æ¥å£
# ================================================================
@app.post("/api/kronos/predict", response_model=PredictResponse, summary="K çº¿é¢„æµ‹")
async def predict_kline(request: PredictRequest, predictor: KronosPredictor = Depends(get_predictor)):
    try:
        if len(request.kline_data) < 10:
            raise HTTPException(status_code=400, detail="å†å²æ•°æ®ä¸è¶³ï¼ˆè‡³å°‘éœ€è¦ 10 æ¡ï¼‰")

        df = pd.DataFrame([x.dict() for x in request.kline_data])
        required_cols = ["open", "high", "low", "close", "volume"]
        for col in required_cols:
            if col not in df.columns:
                raise HTTPException(status_code=400, detail=f"ç¼ºå°‘åˆ—ï¼š{col}")

        # ç”Ÿæˆå†å²æ—¶é—´æˆ³
        delta = parse_timedelta(request.freq)
        end_time = pd.Timestamp.now()
        start_time = end_time - delta * (len(df) - 1)
        df["timestamps"] = pd.date_range(start=start_time, periods=len(df), freq=request.freq)

        # ç”Ÿæˆæœªæ¥é¢„æµ‹æ—¶é—´æˆ³ (ç±»å‹ä¸º DatetimeIndex)
        y_timestamp_index = pd.date_range(
            start=end_time + delta,
            periods=request.pred_len,
            freq=request.freq
        )

        # ================================================================
        
        # 1. å‡†å¤‡æ¨¡å‹è¾“å…¥çš„ dfï¼š
        #    æ ¹æ® Jupyter æµ‹è¯•ï¼Œdf å¿…é¡»åŒ…å«æ—¶é—´æˆ³åˆ—
        #    ä½†æ ¹æ®é”™è¯¯ 1ï¼Œdf å¿…é¡»æ’é™¤å« NaN çš„ 'amount' åˆ—
        model_input_cols = required_cols + ["timestamps"]
        df_model_input = df[model_input_cols]

        # 2. å‡†å¤‡ x_timestamp (ç±»å‹å¿…é¡»æ˜¯ Series)
        x_timestamps_series = pd.Series(df["timestamps"])

        # 3. å‡†å¤‡ y_timestamp (ç±»å‹å¿…é¡»æ˜¯ Seriesï¼Œä¿®å¤ 'dt' é”™è¯¯)
        y_timestamps_series = pd.Series(y_timestamp_index)
        # ================================================================


        # æ¨¡å‹é¢„æµ‹
        with torch.inference_mode():
            pred_df = predictor.predict(
                df=df_model_input,                 # <--- 1. ä½¿ç”¨ç­›é€‰åçš„ DF
                x_timestamp=x_timestamps_series,   # <--- 2. ä½¿ç”¨ x_timestamp Series
                y_timestamp=y_timestamps_series,   # <--- 3. ä½¿ç”¨ y_timestamp Series
                pred_len=request.pred_len,
                T=request.T,
                top_p=request.top_p,               # <--- ç¡®ä¿è¿™é‡Œæ˜¯ top_p
                sample_count=request.sample_count
            )

        # æ ¼å¼åŒ–è¾“å‡º
        result = {
            str(ts): {
                "open": round(float(row["open"]), 2),
                "high": round(float(row["high"]), 2),
                "low": round(float(row["low"]), 2),
                "close": round(float(row["close"]), 2),
                "volume": round(float(row["volume"]), 2),
            }
            for ts, row in pred_df.iterrows()
        }

        return PredictResponse(code=200, message="é¢„æµ‹æˆåŠŸ", data=result)

    except HTTPException as e:
        raise e
    except Exception as e:
        logger.exception("é¢„æµ‹å¤±è´¥ï¼š%s", str(e))
        return PredictResponse(code=500, message=f"é¢„æµ‹å¤±è´¥: {str(e)}", data={})


# ================================================================
# 7. å¥åº·æ£€æŸ¥
# ================================================================
@app.get("/api/health", summary="æœåŠ¡å¥åº·æ£€æŸ¥")
async def health_check():
    return {
        "status": "healthy" if predictor else "uninitialized",
        "device": device,
        "model_loaded": predictor is not None,
        "timestamp": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
    }


# ================================================================
# 8. æœ¬åœ°å¯åŠ¨ï¼ˆä»…è°ƒè¯•ï¼‰
# ================================================================
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app="kronos_api:app",
        host="0.0.0.0",
        port=7661,
        reload=False,
        workers=1
    )
