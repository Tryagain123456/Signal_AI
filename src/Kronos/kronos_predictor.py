import sys
import torch
import numpy as np
import pandas as pd
import logging
import akshare as ak
from datetime import datetime, timedelta
import pandas as pd
import os
import os
from dotenv import load_dotenv
from plotly.subplots import make_subplots
import plotly.graph_objs as go
import numpy as np
import pandas as pd
import math
from typing import Dict, Any, Tuple

# -----------------------------------------------------------------
# 1. æ—¥å¿—é…ç½® (å…¨å±€)
# -----------------------------------------------------------------
log_dir = "./logs"
output_dir = "./output_images_kronos"
os.makedirs(log_dir, exist_ok=True)
os.makedirs(output_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(os.path.join(log_dir, "kronos_prediction.log"), mode='a'),
        logging.StreamHandler()          # <-- æ–°å¢ï¼šå®æ—¶æ‰“å°åˆ°ç»ˆç«¯
    ]
)
logger = logging.getLogger("kronos_predictor")


# -----------------------------------------------------------------
# 2. æ¨¡å‹åŠ è½½ (å…¨å±€å•ä¾‹)
# -----------------------------------------------------------------
predictor = None
def _lazy_load_predictor():
    """æ‡’åŠ è½½ï¼šç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶å†å¯¼å…¥æ¨¡å‹ï¼Œé¿å…å·¥ä½œç›®å½•é—®é¢˜"""
    global predictor
    if predictor is not None:               # å·²åŠ è½½ç›´æ¥è¿”å›
        return

    # æŠŠæ¨¡å‹æ‰€åœ¨ç›®å½•æ’åˆ° sys.pathï¼Œä¿è¯ from model import ... èƒ½æ‰¾åˆ°
    kronos_dir = os.path.dirname(os.path.abspath(__file__))
    if kronos_dir not in sys.path:
        sys.path.insert(0, kronos_dir)

    logger.info("å¼€å§‹åŠ è½½æ¨¡å‹...")
    try:
        current_file_path = os.path.abspath(__file__)
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_file_path)))
        env_path = os.path.join(project_root, ".env")
        load_dotenv(dotenv_path=env_path)

        model_path = os.environ.get("KRONOS_MODEL_PATH")
        print(f"{model_path}\n")
        tokenizer_path = os.environ.get("KRONOS_TOKENIZER_PATH") + "/"
        print(tokenizer_path)
        from model import Kronos, KronosTokenizer, KronosPredictor

        tokenizer = KronosTokenizer.from_pretrained(tokenizer_path)
        model = Kronos.from_pretrained(model_path)
        device = "cuda:2" if torch.cuda.is_available() else "cpu"
        model = model.to(device)
        predictor = KronosPredictor(model, tokenizer, device=device)
        logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {device}")

    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"[Kronos] åŠ è½½å¤±è´¥: {e}")
        predictor = None


# -----------------------------------------------------------------
# 3. æ•°æ®è·å–å‡½æ•°
# -----------------------------------------------------------------
def get_and_process_data(symbol, start_date, end_date, adjust=""):
    """è·å–å¹¶å¤„ç†æ•°æ®ï¼ŒåŒ…æ‹¬é‡å‘½ååˆ—ç­‰æ“ä½œ"""
    try:
        # (æ³¨æ„) start_date å’Œ end_date æœŸæœ›æ˜¯ datetime æˆ– Timestamp å¯¹è±¡
        df = ak.stock_zh_a_hist(
            symbol=symbol,
            period="daily",
            start_date=start_date.strftime("%Y%m%d"),
            end_date=end_date.strftime("%Y%m%d"),
            adjust=adjust
        )

        if df is None or df.empty:
            logger.warning(f"æœªèƒ½è·å–åˆ° {symbol} çš„æ•°æ®ã€‚")
            return pd.DataFrame()

        df = df.rename(columns={
            "æ—¥æœŸ": "date", "å¼€ç›˜": "open", "æœ€é«˜": "high", "æœ€ä½": "low",
            "æ”¶ç›˜": "close", "æˆäº¤é‡": "volume", "æˆäº¤é¢": "amount", "æŒ¯å¹…": "amplitude",
            "æ¶¨è·Œå¹…": "pct_change", "æ¶¨è·Œé¢": "change_amount", "æ¢æ‰‹ç‡": "turnover"
        })

        df["date"] = pd.to_datetime(df["date"])
        
        keep_cols = ['date', 'open', 'close', 'high', 'low', 'volume', 'amount']
        df = df[keep_cols].copy() 
        
        float_cols = ['open', 'close', 'high', 'low', 'volume', 'amount']
        df[float_cols] = df[float_cols].astype(float)
        
        return df

    except Exception as e:
        logger.error(f"è·å–æ•°æ®æ—¶å‡ºé”™ (symbol={symbol}): {e}")
        return pd.DataFrame()


# -----------------------------------------------------------------
# 4. æ•°æ®æ¸…ç†å‡½æ•° 
# -----------------------------------------------------------------
def clear_data(df: pd.DataFrame) -> pd.DataFrame:
    """é«˜çº§æ•°æ®æ¸…ç†ç®—æ³• - 3ÏƒåŸåˆ™ + OHLCé€»è¾‘ä¿®å¤ + Logå˜æ¢"""
    try:
        if df.empty:
            return df
            
        original_count = len(df)
        logger.info(f"å¼€å§‹æ•°æ®æ¸…ç† (åŸå§‹ {original_count} æ¡)...")
        
        # 1. å¼‚å¸¸å€¼æ£€æµ‹å’Œç§»é™¤ (3ÏƒåŸåˆ™)
        for col in ['open', 'high', 'low', 'close']:
            mean_val = df[col].mean()
            std_val = df[col].std()
            lower_bound = mean_val - 3 * std_val
            upper_bound = mean_val + 3 * std_val
            
            before_count = len(df)
            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
            removed_count = before_count - len(df)
            
            if removed_count > 0:
                logger.info(f"  - {col} åˆ—ç§»é™¤ {removed_count} ä¸ªå¼‚å¸¸å€¼")
        
        # 2. OHLCæ•°æ®é€»è¾‘å…³ç³»ä¿®å¤
        df['high'] = df[['open', 'close', 'high']].max(axis=1)
        df['low'] = df[['open', 'close', 'low']].min(axis=1)
        logger.info("  - OHLC é€»è¾‘å…³ç³»å·²ä¿®å¤")
        
        # 3. æˆäº¤é‡æ•°æ®å¤„ç†
        if 'volume' in df.columns:
            before_count = len(df)
            df = df[df['volume'] > 0]
            zero_volume_removed = before_count - len(df)
            
            volume_q99 = df['volume'].quantile(0.99)
            before_count = len(df)
            df = df[df['volume'] <= volume_q99]
            extreme_volume_removed = before_count - len(df)
            
            logger.info(f"  - ç§»é™¤é›¶æˆäº¤é‡: {zero_volume_removed} æ¡")
            logger.info(f"  - ç§»é™¤æç«¯æˆäº¤é‡: {extreme_volume_removed} æ¡")

            # (å…³é”®æ–°å¢) 4. å¯¹æˆäº¤é‡å’Œæˆäº¤é¢è¿›è¡Œ log1p å˜æ¢ (æ§åˆ¶é‡çº²)
            # è¿™å¯ä»¥ "æ§åˆ¶" æç«¯å¤§å€¼ï¼Œä½¿å…¶åˆ†å¸ƒæ›´å¹³æ»‘
            df['volume'] = np.log1p(df['volume'])
            if 'amount' in df.columns:
                 df['amount'] = np.log1p(df['amount'])
            logger.info("  - (å…³é”®) å·²å¯¹ 'volume' å’Œ 'amount' åº”ç”¨ log1p å˜æ¢")

        # 5. æ•°æ®æ¸…ç†æ€»ç»“
        cleaned_count = len(df)
        removed_total = original_count - cleaned_count
        removal_rate = (removed_total / original_count * 100) if original_count > 0 else 0
        
        logger.info(f"æ•°æ®æ¸…ç†å®Œæˆ: åŸå§‹ {original_count} æ¡ â†’ æ¸…ç†å {cleaned_count} æ¡ (ç§»é™¤ {removal_rate:.2f}%)")
        
        return df.reset_index(drop=True)
        
    except Exception as e:
        logger.error(f"æ•°æ®æ¸…ç†å‡ºé”™: {e}")
        return df.reset_index(drop=True)




# -----------------------------------------------------------------
# 5. ç»˜å›¾å‡½æ•° 
# -----------------------------------------------------------------
def build_prediction_figure(history_data, predict_data, lookback=400):
    """åˆ›å»ºhistory_dataä¸predict_dataçš„ä¸“ä¸šå¯¹æ¯”å›¾"""
    
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=('ğŸ“ˆ ä»·æ ¼é¢„æµ‹å¯¹æ¯”', 'ğŸ“Š æˆäº¤é‡é¢„æµ‹å¯¹æ¯”'),
        vertical_spacing=0.1,
        row_heights=[0.7, 0.3],
        specs=[[{"secondary_y": False}], [{"secondary_y": False}]]
    )
    
    #  å¤åˆ¶æ•°æ®ï¼Œå¹¶åè½¬å†å²æ•°æ®çš„ log å˜æ¢
    hist_data = history_data.tail(lookback).copy()
    
    if 'volume' in hist_data.columns:
        # æ£€æŸ¥æ•°æ®æ˜¯å¦çœŸçš„è¢«logè¿‡ (ä¾‹å¦‚ï¼Œæœ€å¤§å€¼ < 30)
        # log1p(100000) approx 11.5
        if hist_data['volume'].max() < 30: 
            hist_data['volume'] = np.expm1(hist_data['volume'])
            logger.info("  - (ç»˜å›¾) å·²åè½¬ history_data 'volume' çš„ log å˜æ¢ï¼Œç”¨äºç»˜å›¾")

    # å†å²Kçº¿å›¾ (ç¬¬ä¸€è¡Œ)
    fig.add_trace(
        go.Candlestick(
            x=hist_data['timestamps'],
            open=hist_data['open'],
            high=hist_data['high'],
            low=hist_data['low'],
            close=hist_data['close'],
            name="ğŸ“Š å†å²Kçº¿",
            increasing_line_color='#26a69a',
            decreasing_line_color='#ef5350',
            opacity=0.8
        ),
        row=1, col=1
    )
    
    # é¢„æµ‹Kçº¿å›¾ (ç¬¬ä¸€è¡Œ)
    if predict_data is not None and not predict_data.empty:
        pred_times = predict_data.index
        
        fig.add_trace(
            go.Candlestick(
                x=pred_times,
                open=predict_data['open'],
                high=predict_data['high'],
                low=predict_data['low'],
                close=predict_data['close'],
                name="ğŸ”® AIé¢„æµ‹Kçº¿",
                increasing_line_color='#66bb6a',
                decreasing_line_color='#ff7043',
                opacity=0.9
            ),
            row=1, col=1
        )
        
        if not hist_data.empty:
            connection_x = [hist_data['timestamps'].iloc[-1], pred_times[0]]
            connection_y = [hist_data['close'].iloc[-1], predict_data['open'].iloc[0]]
            
            fig.add_trace(
                go.Scatter(
                    x=connection_x, y=connection_y, mode='lines',
                    name='é¢„æµ‹è¿æ¥', line=dict(color='red', width=2, dash='dash'),
                    showlegend=False
                ),
                row=1, col=1
            )
    
    # å†å²æˆäº¤é‡ (ç¬¬äºŒè¡Œ)
    if 'volume' in hist_data.columns:
        fig.add_trace(
            go.Bar(
                x=hist_data['timestamps'],
                y=hist_data['volume'], # (å…³é”®) è¿™æ˜¯åè½¬åçš„ volume
                name="ğŸ“Š å†å²æˆäº¤é‡",
                marker_color='lightblue',
                opacity=0.7,
                hovertemplate='æ—¶é—´: %{x}<br>æˆäº¤é‡: %{y:,.0f}<extra></extra>'
            ),
            row=2, col=1
        )
    
    # é¢„æµ‹æˆäº¤é‡ (ç¬¬äºŒè¡Œ)
    if predict_data is not None and 'volume' in predict_data.columns:
        fig.add_trace(
            go.Bar(
                x=pred_times, # type: ignore
                y=predict_data['volume'], # (å…³é”®) è¿™æ˜¯åè½¬åçš„ volume
                name="ğŸ”® é¢„æµ‹æˆäº¤é‡",
                marker_color='orange',
                opacity=0.7,
                hovertemplate='æ—¶é—´: %{x}<br>é¢„æµ‹æˆäº¤é‡: %{y:,.0f}<extra></extra>'
            ),
            row=2, col=1
        )
    
    # å¸ƒå±€é…ç½®
    title_end_date = "..."
    if predict_data is not None and not predict_data.empty:
        title_end_date = pred_times[-1].date() # type: ignore
        
    fig.update_layout(
        title=f"ğŸ¯ Kronos AIè‚¡ç¥¨é¢„æµ‹ç»“æœåˆ†æ ({hist_data['date'].iloc[0].date()} - {title_end_date})",
        template="plotly_white",
        height=800,
        showlegend=True,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
    )
    
    for row in [1, 2]:
        fig.update_xaxes(
            type='date', showgrid=True, gridcolor='lightgray',
            row=row, col=1
        )
    
    fig.update_xaxes(title_text="æ—¶é—´", row=2, col=1)
    fig.update_yaxes(title_text="ä»·æ ¼ (Â¥)", row=1, col=1)
    fig.update_yaxes(title_text="æˆäº¤é‡", row=2, col=1)
    
    return fig






# -----------------------------------------------------------------
# 6. ä¸»é¢„æµ‹å‡½æ•° 
# -----------------------------------------------------------------
from typing import Optional

# def kronos_predict(symbol: str, start_date_str: Optional[str] = None, end_date_str: Optional[str] = None, pred_len: int = 90, T: float = 1.0, top_p: float = 0.9, sample_count: int = 1) :
#     """
#     ä¸»é¢„æµ‹å‡½æ•°ï¼šè·å–ã€æ¸…ç†ã€é¢„æµ‹å¹¶ç»˜å›¾ã€‚
#
#     (æ–°å¢) é»˜è®¤ä½¿ç”¨è¿‘3å¹´æ•°æ®é¢„æµ‹æœªæ¥90å¤©ã€‚
#     å¦‚æœ `start_date_str` æˆ– `end_date_str` ä¸º Noneï¼Œå°†å¯ç”¨é»˜è®¤é€»è¾‘ã€‚
#
#     (æ–°å¢) æ–°å¢å‚æ•° T, top_p, sample_count ç”¨äºæ§åˆ¶æ¨¡å‹é¢„æµ‹è¡Œä¸ºã€‚
#     """
#     _lazy_load_predictor()
#     if predictor is None:
#         logger.error("é¢„æµ‹å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œé¢„æµ‹ã€‚")
#         return None, None
#     # --- é»˜è®¤æ—¥æœŸé€»è¾‘å¤„ç† ---
#     if end_date_str is None:
#         end_ts = datetime.now()
#         end_date_str_log = end_ts.strftime("%Y-%m-%d")
#         logger.info(f"æœªæŒ‡å®š end_date_strï¼Œä½¿ç”¨é»˜è®¤å€¼ (ä»Šå¤©): {end_date_str_log}")
#     else:
#         end_ts = pd.Timestamp(end_date_str)
#         end_date_str_log = end_date_str
#
#     if start_date_str is None:
#         start_ts = end_ts - timedelta(days=3*365)
#         start_date_str_log = start_ts.strftime("%Y-%m-%d")
#         logger.info(f"æœªæŒ‡å®š start_date_strï¼Œä½¿ç”¨é»˜è®¤å€¼ (3å¹´å‰): {start_date_str_log}")
#     else:
#         start_ts = pd.Timestamp(start_date_str)
#         start_date_str_log = start_date_str
#     # --- (ä¿®æ”¹ç»“æŸ) ---
#
#     if predictor is None:
#         logger.error("é¢„æµ‹å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œé¢„æµ‹ã€‚")
#         return None, None
#
#     logger.info(f"--- [START] å¼€å§‹ä¸º {symbol} æ‰§è¡Œé¢„æµ‹ (å‘¨æœŸ: {start_date_str_log} to {end_date_str_log}, é¢„æµ‹ {pred_len} å¤©) ---")
#
#     # 1. æ•°æ®è·å–
#     df_raw = get_and_process_data(symbol, start_ts, end_ts)
#
#     if df_raw.empty:
#         logger.error(f"è·å–æ•°æ®å¤±è´¥ {symbol}ã€‚")
#         return None, None
#
#     # 2. æ•°æ®æ¸…ç†
#     df_cleaned = clear_data(df_raw.copy())
#
#     if df_cleaned.empty or len(df_cleaned) < 10:
#         logger.error(f"æ¸…ç†åæ•°æ®ä¸è¶³10æ¡ {symbol} (å‰©ä½™ {len(df_cleaned)} æ¡)ï¼Œæ— æ³•é¢„æµ‹ã€‚")
#         return None, None
#
#
#     # 3. åœ¨æ¸…ç†æ•°æ® *ä¹‹å* åˆ›å»ºæ—¶é—´æˆ³
#     #  æˆ‘ä»¬å¿…é¡»ä½¿ç”¨ 'date' åˆ—ä¸­çš„ *å®é™…äº¤æ˜“æ—¥* ä½œä¸ºæ—¶é—´æˆ³ï¼Œ
#     df_cleaned["timestamps"] = df_cleaned["date"]
#
#     # ç¡®ä¿ 'timestamps' åˆ—æ˜¯ pd.Timestamp ç±»å‹
#     df_cleaned["timestamps"] = pd.to_datetime(df_cleaned["timestamps"])
#
#     logger.info(f"å·²ä½¿ç”¨ 'date' åˆ—ä¸­çš„å®é™…äº¤æ˜“æ—¥ä½œä¸º 'timestamps'ã€‚")
#
#
#     # 4. å‡†å¤‡é¢„æµ‹è¾“å…¥
#     #  ç°åœ¨ last_timestamp æ˜¯ *çœŸæ­£* çš„æœ€åä¸€ä¸ªäº¤æ˜“æ—¥
#     last_timestamp = df_cleaned["timestamps"].iloc[-1]
#     logger.info(f"å†å²æ•°æ®ä¸­çš„æœ€åä¸€ä¸ª 'timestamp' æ˜¯: {last_timestamp.date()}")
#
#     x_timestamp = pd.Series(df_cleaned["timestamps"])
#
#     #  é¢„æµ‹çš„æ—¶é—´æˆ³ 'y_timestamp' åº”è¯¥è·³è¿‡å‘¨æœ«ã€‚
#     # æˆ‘ä»¬ä½¿ç”¨ 'freq="B"' (Business Day) æ¥ç”Ÿæˆæœªæ¥çš„ *äº¤æ˜“æ—¥*ã€‚
#     y_timestamp = pd.Series(
#         pd.date_range(
#             start=last_timestamp + pd.Timedelta(days=1), # ä»æœ€åå†å²æ—¥æœŸçš„ *ç¬¬äºŒå¤©* å¼€å§‹
#             periods=pred_len,
#             freq="B"  # <--- ä½¿ç”¨ "B" (Business Day, å‘¨ä¸€è‡³å‘¨äº”)
#         )
#     )
#
#
#     logger.info(f"é¢„æµ‹å°†ä» {y_timestamp.iloc[0].date()} å¼€å§‹ (ä½¿ç”¨ 'B' é¢‘ç‡è·³è¿‡å‘¨æœ«)ã€‚")
#
#     # æ˜¾å¼é€‰æ‹©æ¨¡å‹éœ€è¦çš„åˆ—
#     model_input_cols = ['open', 'high', 'low', 'close', 'volume', 'timestamps']
#     df_for_model = df_cleaned[model_input_cols]
#
#     logger.info(f"å‡†å¤‡è°ƒç”¨é¢„æµ‹å™¨... å†å²æ•°æ® {len(df_for_model)} æ¡ (Volume å·² Log å˜æ¢)ã€‚")
#
#     # 5. è°ƒç”¨ predict
#     try:
#         with torch.inference_mode():
#             pred_df = predictor.predict(
#                 df=df_for_model,
#                 x_timestamp=x_timestamp,
#                 y_timestamp=y_timestamp,
#                 pred_len=pred_len,
#                 T=T,
#                 top_p=top_p,
#                 sample_count=sample_count
#             )
#
#         # åè½¬æˆäº¤é‡çš„ log1p å˜æ¢
#         if 'volume' in pred_df.columns:
#             pred_df['volume'] = np.expm1(pred_df['volume'])
#             logger.info("  - (å…³é”®) å·²åè½¬é¢„æµ‹ç»“æœ 'volume' çš„ log1p å˜æ¢ (expm1)")
#
#         logger.info("é¢„æµ‹æˆåŠŸã€‚")
#         print("\n--- é¢„æµ‹ç»“æœ (Head) ---")
#         print(pred_df[["open", "high", "low", "close", "volume"]].head())
#         print("------------------------\n")
#
#         # 6. ç»˜å›¾
#         logger.info("ç”Ÿæˆé¢„æµ‹å¯¹æ¯”å›¾...")
#         fig = build_prediction_figure(df_cleaned, pred_df)
#
#         #  ä¿å­˜å›¾è¡¨åˆ°æœ¬åœ°æ–‡ä»¶ ---
#         try:
#             global output_dir
#
#             #  safe_end_date_str ç°åœ¨åŸºäº end_ts (å³ 'today' æˆ–æŒ‡å®šçš„ç»“æŸæ—¥æœŸ)
#             safe_end_date_str = end_ts.strftime("%Y%m%d")
#             filename = f"{symbol}_{safe_end_date_str}_pred_{pred_len}d.html"
#             save_path = os.path.join(output_dir, filename)
#
#             fig.write_html(save_path)
#
#             logger.info(f"é¢„æµ‹å›¾è¡¨å·²æˆåŠŸä¿å­˜åˆ°: {save_path}")
#             print(f"f é¢„æµ‹å›¾è¡¨å·²æˆåŠŸä¿å­˜åˆ°: {save_path}")
#
#         except Exception as e:
#             logger.error(f"ä¿å­˜ HTML å›¾è¡¨å¤±è´¥: {e}")
#             print(f"âŒ ä¿å­˜ HTML å›¾è¡¨å¤±è´¥: {e}")
#
#
#         logger.info(f"--- [END] é¢„æµ‹å®Œæˆ {symbol} ---")
#         return df_cleaned, pred_df
#
#     except Exception as e:
#         logger.exception(f"é¢„æµ‹æˆ–ç»˜å›¾è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
#         return df_cleaned, None
#





# å¸¸é‡ï¼šæ­£æ€åˆ†ä½ç‚¹ï¼ˆå•å°¾ï¼‰
Z_95 = -1.645
Z_99 = -2.33

def analyze_prediction_df(pred_df: pd.DataFrame, freq_per_year: int = 252, bootstrap_iters: int = 1000) -> Dict[str, Any]:
    """
    è¾“å…¥:
      pred_df: é¢„æµ‹ç»“æœ DataFrameï¼Œç´¢å¼•ä¸º pd.DatetimeIndexï¼Œå¿…é¡»åŒ…å« 'close' åˆ—ï¼ˆå¯å« volumeï¼‰
      freq_per_year: å¹´äº¤æ˜“æ—¥æ•°ï¼ˆè‚¡ç¥¨é€šå¸¸ç”¨252ï¼‰
      bootstrap_iters: è‹¥åªæœ‰å•ä¸€è·¯å¾„ï¼Œbootstrap ç”¨äºä¼°è®¡ä¸ç¡®å®šæ€§

    è¿”å›:
      metrics: å­—å…¸ï¼ŒåŒ…å«ç»Ÿè®¡é‡ã€é£é™©æŒ‡æ ‡ã€è¶‹åŠ¿åˆ¤æ–­ã€ä»“ä½å»ºè®®ã€æŠ¥å‘Šæ–‡æœ¬ç­‰
    """
    result: Dict[str, Any] = {}
    df = pred_df.copy().sort_index()

    # 1) åŸºæœ¬æ£€æŸ¥
    if 'close' not in df.columns:
        raise ValueError("pred_df å¿…é¡»åŒ…å« 'close' åˆ—")

    # 2) è®¡ç®—å¯¹æ•°æ”¶ç›Šï¼ˆç›¸å¯¹å˜åŒ–ï¼‰
    df['log_ret'] = np.log(df['close'] / df['close'].shift(1))
    df = df.dropna(subset=['log_ret'])
    n = len(df)
    if n == 0:
        raise ValueError("é¢„æµ‹æ•°æ®é•¿åº¦ä¸è¶³ä»¥è®¡ç®—æ”¶ç›Š")

    # 3) åŸºæœ¬ç»Ÿè®¡
    mean_daily = df['log_ret'].mean()
    std_daily = df['log_ret'].std(ddof=1)
    sharpe_daily = mean_daily / (std_daily + 1e-12)

    annualized_return = (np.exp(mean_daily * freq_per_year) - 1)
    annualized_vol = std_daily * np.sqrt(freq_per_year)

    result['n_days'] = int(n)
    result['mean_daily'] = float(mean_daily)
    result['std_daily'] = float(std_daily)
    result['annualized_return'] = float(annualized_return)
    result['annualized_vol'] = float(annualized_vol)
    result['sharpe_approx'] = float(sharpe_daily * np.sqrt(freq_per_year))

    # 4) ç´¯ç§¯ä¸æœ€å¤§å›æ’¤
    df['cum_ret'] = np.exp(df['log_ret'].cumsum())  # ç´¯ç§¯ä¹˜ç§¯å½¢å¼
    df['cum_peak'] = df['cum_ret'].cummax()
    df['drawdown'] = df['cum_ret'] / df['cum_peak'] - 1
    max_drawdown = df['drawdown'].min()
    result['max_drawdown'] = float(max_drawdown)

    # å›æ’¤æŒç»­æœŸï¼ˆæœ€é•¿è¿ç»­ä¸‹è·Œå¤©æ•°ï¼‰
    dd_mask = df['drawdown'] < 0
    longest_drawdown_duration = 0
    cur = 0
    for v in dd_mask:
        if v:
            cur += 1
            longest_drawdown_duration = max(longest_drawdown_duration, cur)
        else:
            cur = 0
    result['max_drawdown_duration_days'] = int(longest_drawdown_duration)

    # 5) VaR ä¸ ESï¼ˆå†å²æ³• + å‚æ•°æ³•ï¼‰
    # a) å†å² VaRï¼ˆä»¥ log returnsï¼‰
    var_95_hist = np.percentile(df['log_ret'], 5)
    var_99_hist = np.percentile(df['log_ret'], 1)

    # b) å‚æ•°åŒ– VaRï¼ˆæ­£æ€å‡è®¾ï¼‰
    var_95_param = mean_daily + Z_95 * std_daily
    var_99_param = mean_daily + Z_99 * std_daily

    # ESï¼ˆå†å²ï¼‰ï¼šå¹³å‡ä½äº VaR çš„æŸå¤±
    es_95_hist = df['log_ret'][df['log_ret'] <= var_95_hist].mean()
    es_99_hist = df['log_ret'][df['log_ret'] <= var_99_hist].mean()

    result.update({
        'VaR_95_hist': float(var_95_hist),
        'VaR_99_hist': float(var_99_hist),
        'VaR_95_param': float(var_95_param),
        'VaR_99_param': float(var_99_param),
        'ES_95_hist': float(es_95_hist) if not np.isnan(es_95_hist) else None,
        'ES_99_hist': float(es_99_hist) if not np.isnan(es_99_hist) else None
    })

    # 6) è¶‹åŠ¿ä¸åŠ¨é‡
    # çº¿æ€§å›å½’æ–œç‡ (å¯¹ log close åšå›å½’ï¼Œä»¥å¤©åºå·ä½œ x)
    y = np.log(df['close']).values # type: ignore
    x = np.arange(len(y))
    if len(x) >= 2:
        A = np.vstack([x, np.ones_like(x)]).T
        slope, intercept = np.linalg.lstsq(A, y, rcond=None)[0]
        # å°†æ–œç‡è½¬æ¢æˆå¹´åŒ–æ”¶ç›Šè¿‘ä¼¼: exp(slope * freq) - 1
        approx_annual_slope = math.exp(slope * freq_per_year) - 1
    else:
        slope = 0.0
        approx_annual_slope = 0.0

    # æœ€è¿‘ 20/60 å¤©åŠ¨é‡
    mom_20 = df['log_ret'].rolling(window=min(20, len(df))).sum().iloc[-1]
    mom_60 = df['log_ret'].rolling(window=min(60, len(df))).sum().iloc[-1]

    result.update({
        'trend_slope_daily': float(slope),
        'trend_approx_annual_return': float(approx_annual_slope),
        'momentum_20d': float(mom_20),
        'momentum_60d': float(mom_60)
    })

    # 7) æ³¢åŠ¨ç‡åˆ¶åº¦ï¼ˆvol regimeï¼‰
    hist_vol_ma = df['log_ret'].rolling(window=min(63, len(df))).std().mean()  # approx 3-month mean vol
    current_vol = std_daily
    vol_regime_ratio = current_vol / (hist_vol_ma + 1e-12)
    result['vol_regime_ratio'] = float(vol_regime_ratio)

    # 8) ç½®ä¿¡åŒºé—´ä¼°è®¡ï¼ˆè‹¥å­˜åœ¨å¤šä¸ªæ ·æœ¬è·¯å¾„å¯ç”¨æ ·æœ¬ç›´æ¥ä¼°è®¡ï¼›å¦åˆ™ç”¨ bootstrapï¼‰
    # æ£€æŸ¥æ˜¯å¦æœ‰å¤šè·¯å¾„ï¼špred_df å¯èƒ½æ¥è‡ª sample_count>1 å¹¶åˆå¹¶åœ¨ä¸€å¼ è¡¨ï¼›æˆ‘ä»¬é»˜è®¤æ­¤å‡½æ•°æ¥æ”¶å•ä¸€è·¯å¾„ã€‚
    # ä½¿ç”¨ bootstrap å¯¹æœªæ¥ T ç»ˆå€¼è¿›è¡Œä¸ç¡®å®šæ€§ä¼°è®¡ï¼ˆå¯¹ log_ret è¿›è¡Œé‡æŠ½æ ·ï¼‰
    final_price = df['close'].iloc[-1]
    boot_final_prices = []
    rng = np.random.default_rng(42)
    for _ in range(bootstrap_iters):
        resampled = rng.choice(df['log_ret'].values, size=len(df), replace=True) # type: ignore
        final = final_price * np.exp(resampled.sum())
        boot_final_prices.append(final)
    boot_final_prices = np.array(boot_final_prices)
    ci_lower = np.percentile(boot_final_prices, 2.5)
    ci_upper = np.percentile(boot_final_prices, 97.5)
    median_final = np.median(boot_final_prices)

    result.update({
        'final_price_median_boot': float(median_final),
        'final_price_95ci_lower': float(ci_lower),
        'final_price_95ci_upper': float(ci_upper)
    })

    # 9) ä»“ä½å»ºè®®ï¼ˆåŸºäºè§„åˆ™ï¼‰
    # è§„åˆ™ç¤ºä¾‹ï¼ˆå¯æŒ‰éœ€æ±‚è°ƒå‚ï¼‰
    # - å¼ºä¹°å…¥: trend positive, low vol_regime_ratio < 0.9, max_drawdown small > -0.05
    # - ä¹°å…¥: trend moderately positive, vol moderate
    # - ä¸­æ€§: trend neutral or vol high
    # - å–å‡º: trend negative or max_drawdown > 0.1 or VaR_95_param < -0.02
    score = 0.0
    # trend contribution
    if approx_annual_slope > 0.05:
        score += 1.0
    elif approx_annual_slope > 0.02:
        score += 0.5
    elif approx_annual_slope < -0.02:
        score -= 0.5
    elif approx_annual_slope < -0.05:
        score -= 1.0

    # vol penalty
    if vol_regime_ratio < 0.9:
        score += 0.5
    elif vol_regime_ratio > 1.2:
        score -= 0.7

    # drawdown penalty
    if max_drawdown < -0.15:
        score -= 1.0
    elif max_drawdown < -0.08:
        score -= 0.5

    # VaR penalty (daily)
    if result['VaR_95_param'] < -0.03:
        score -= 0.5

    # map score to advice
    if score >= 1.5:
        advice = "strong_buy"
        position_suggestion = "aggressive"  # å¯è€ƒè™‘è¾ƒå¤§ä»“ä½
    elif score >= 0.5:
        advice = "buy"
        position_suggestion = "moderate"
    elif score > -0.5:
        advice = "hold"
        position_suggestion = "neutral"
    elif score > -1.5:
        advice = "reduce"
        position_suggestion = "defensive"
    else:
        advice = "sell"
        position_suggestion = "close"

    result.update({
        'score': float(score),
        'advice': advice,
        'position_suggestion': position_suggestion
    })

    # 10) ç”Ÿæˆè‡ªç„¶è¯­è¨€ç®€çŸ­æŠ¥å‘Šï¼ˆå¯ç›´æ¥å†™å…¥ AgentStateï¼‰
    report_lines = []
    report_lines.append(f"é¢„æµ‹åŒºé—´é•¿åº¦: {n} ä¸ªäº¤æ˜“æ—¥ï¼Œæœ€ç»ˆé¢„æµ‹ä»·: {df['close'].iloc[-1]:.4f}")
    report_lines.append(f"è¿‘ä¼¼å¹´åŒ–æ”¶ç›Š: {annualized_return:.2%}ï¼Œå¹´åŒ–æ³¢åŠ¨: {annualized_vol:.2%}")
    report_lines.append(f"æœ€å¤§å›æ’¤: {max_drawdown:.2%}ï¼Œæœ€é•¿å›æ’¤æŒç»­æœŸ: {longest_drawdown_duration} æ—¥")
    report_lines.append(f"VaR(95%, param): {result['VaR_95_param']:.2%} / VaR(95%, hist): {result['VaR_95_hist']:.2%}")
    report_lines.append(f"è¶‹åŠ¿æ–œç‡(å¹´åŒ–è¿‘ä¼¼): {approx_annual_slope:.2%}ï¼Œ20æ—¥åŠ¨é‡: {mom_20:.2%}")
    report_lines.append(f"æ³¢åŠ¨ç‡åˆ¶åº¦æ¯” (å½“å‰/å†å²3æœˆå‡): {vol_regime_ratio:.2f}")
    report_lines.append(f"ä»“ä½å»ºè®®: {advice} (æ¡£ä½: {position_suggestion})")
    report_lines.append(f"ä»·æ ¼ 95% CI (bootstrap): [{ci_lower:.4f}, {ci_upper:.4f}]")

    result['text_report'] = "\n".join(report_lines)

    # 11) è¿”å›ç»“æ„
    result['summary_table'] = {
        'last_price': float(df['close'].iloc[-1]),
        'median_final_price_boot': float(median_final),
        'ci_95_lower': float(ci_lower),
        'ci_95_upper': float(ci_upper),
        'advice': advice,
        'position': position_suggestion
    }

    return result


#
# # -----------------------------------------------------------------
# # 7. æ‰§è¡Œå…¥å£ (å…³é”®ä¿®æ”¹)
# # -----------------------------------------------------------------
# if __name__ == "__main__":
#
#     # # --- ç¤ºä¾‹è°ƒç”¨ 1 (ä½¿ç”¨ç‰¹å®šæ—¥æœŸï¼Œè¦†ç›–é»˜è®¤å€¼) ---
#     logger.info("--- [ç¤ºä¾‹ 1] è¿è¡Œç‰¹å®šæ—¥æœŸé¢„æµ‹ (601318) ---")
#     hist_df_1, pred_df_1 = kronos_predict(
#         symbol="601519", # å¤§æ™ºæ…§
#         start_date_str="2020-01-01",
#         end_date_str="2025-05-15",
#         pred_len=100  # è¦†ç›–é»˜è®¤çš„ 90
#     )
#     print(pred_df_1)
#     result = analyze_prediction_df(pred_df_1, freq_per_year=252, bootstrap_iters=2000) # type: ignore
#     print(result)
#     logger.info("--- [ç¤ºä¾‹ 1] å®Œæˆ ---\n")
#
#     # --- ç¤ºä¾‹è°ƒç”¨ 2 (ä½¿ç”¨é»˜è®¤æ—¥æœŸï¼šè¿‘3å¹´ -> é¢„æµ‹ 90 å¤©) ---
#     logger.info("--- [ç¤ºä¾‹ 2] è¿è¡Œé»˜è®¤æ—¥æœŸé¢„æµ‹ (300750) ---")
#     #  ä½¿ç”¨åŸå…ˆè¢«æ³¨é‡Šæ‰çš„ '300750' ä½œä¸ºæ–°é»˜è®¤é€»è¾‘çš„ç¤ºä¾‹
#     # ä¸ä¼ é€’ start/end/pred_len å‚æ•°ï¼Œå°†è‡ªåŠ¨ä½¿ç”¨é»˜è®¤å€¼
#     # hist_df_2, pred_df_2 = kronos_predict(
#     #     symbol="601519" # å¤§æ™ºæ…§
#     #     # start_date_str=None, (ä½¿ç”¨é»˜è®¤å€¼: 3å¹´å‰)
#     #     # end_date_str=None, (ä½¿ç”¨é»˜è®¤å€¼: ä»Šå¤©)
#     #     # pred_len=90 (ä½¿ç”¨é»˜è®¤å€¼)
#     # )
#     logger.info("--- [ç¤ºä¾‹ 2] å®Œæˆ ---\n")


# -----------------------------------------------------------------
# 6. ä¸»é¢„æµ‹å‡½æ•° (å·²ä¿®æ”¹)
# -----------------------------------------------------------------
from typing import Optional, Tuple
import plotly.graph_objs as go  # ç¡®ä¿ go å·²å¯¼å…¥


def kronos_predict(
        symbol: str,
        start_date_str: Optional[str] = None,
        end_date_str: Optional[str] = None,
        pred_len: int = 90,
        T: float = 1.0,
        top_p: float = 0.9,
        sample_count: int = 1
) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame], Optional[go.Figure]]:  # <--- ä¿®æ”¹è¿”å›ç±»å‹
    """
    ä¸»é¢„æµ‹å‡½æ•°ï¼šè·å–ã€æ¸…ç†ã€é¢„æµ‹å¹¶ç»˜å›¾ã€‚

    (å·²ä¿®æ”¹) ç°åœ¨è¿”å› (history_df, prediction_df, plotly_figure_object)
    """
    _lazy_load_predictor()
    if predictor is None:
        logger.error("é¢„æµ‹å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œé¢„æµ‹ã€‚")
        return None, None, None  # <--- [ä¿®æ”¹] è¿”å› 3 ä¸ªå€¼

    # --- é»˜è®¤æ—¥æœŸé€»è¾‘å¤„ç† ---
    if end_date_str is None:
        end_ts = datetime.now()
        end_date_str_log = end_ts.strftime("%Y-%m-%d")
        logger.info(f"æœªæŒ‡å®š end_date_strï¼Œä½¿ç”¨é»˜è®¤å€¼ (ä»Šå¤©): {end_date_str_log}")
    else:
        end_ts = pd.Timestamp(end_date_str)
        end_date_str_log = end_date_str

    if start_date_str is None:
        start_ts = end_ts - timedelta(days=3 * 365)
        start_date_str_log = start_ts.strftime("%Y-%m-%d")
        logger.info(f"æœªæŒ‡å®š start_date_strï¼Œä½¿ç”¨é»˜è®¤å€¼ (3å¹´å‰): {start_date_str_log}")
    else:
        start_ts = pd.Timestamp(start_date_str)
        start_date_str_log = start_date_str

        # --- (å¤šä½™çš„æ£€æŸ¥ï¼Œä½†ä¿æŒä¸€è‡´) ---
    if predictor is None:
        logger.error("é¢„æµ‹å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œé¢„æµ‹ã€‚")
        return None, None, None  # <--- [ä¿®æ”¹] è¿”å› 3 ä¸ªå€¼

    logger.info(
        f"--- [START] å¼€å§‹ä¸º {symbol} æ‰§è¡Œé¢„æµ‹ (å‘¨æœŸ: {start_date_str_log} to {end_date_str_log}, é¢„æµ‹ {pred_len} å¤©) ---")

    # 1. æ•°æ®è·å–
    df_raw = get_and_process_data(symbol, start_ts, end_ts)

    if df_raw.empty:
        logger.error(f"è·å–æ•°æ®å¤±è´¥ {symbol}ã€‚")
        return None, None, None  # <--- [ä¿®æ”¹] è¿”å› 3 ä¸ªå€¼

    # 2. æ•°æ®æ¸…ç†
    df_cleaned = clear_data(df_raw.copy())

    if df_cleaned.empty or len(df_cleaned) < 10:
        logger.error(f"æ¸…ç†åæ•°æ®ä¸è¶³10æ¡ {symbol} (å‰©ä½™ {len(df_cleaned)} æ¡)ï¼Œæ— æ³•é¢„æµ‹ã€‚")
        # [ä¿®æ”¹] å³ä½¿å¤±è´¥ä¹Ÿè¿”å›æ¸…ç†åçš„dfï¼Œä½† pred å’Œ fig ä¸º None
        return df_cleaned, None, None

        # 3. åœ¨æ¸…ç†æ•°æ® *ä¹‹å* åˆ›å»ºæ—¶é—´æˆ³
    df_cleaned["timestamps"] = df_cleaned["date"]
    df_cleaned["timestamps"] = pd.to_datetime(df_cleaned["timestamps"])
    logger.info(f"å·²ä½¿ç”¨ 'date' åˆ—ä¸­çš„å®é™…äº¤æ˜“æ—¥ä½œä¸º 'timestamps'ã€‚")

    # 4. å‡†å¤‡é¢„æµ‹è¾“å…¥
    last_timestamp = df_cleaned["timestamps"].iloc[-1]
    logger.info(f"å†å²æ•°æ®ä¸­çš„æœ€åä¸€ä¸ª 'timestamp' æ˜¯: {last_timestamp.date()}")

    x_timestamp = pd.Series(df_cleaned["timestamps"])
    y_timestamp = pd.Series(
        pd.date_range(
            start=last_timestamp + pd.Timedelta(days=1),
            periods=pred_len,
            freq="B"
        )
    )

    logger.info(f"é¢„æµ‹å°†ä» {y_timestamp.iloc[0].date()} å¼€å§‹ (ä½¿ç”¨ 'B' é¢‘ç‡è·³è¿‡å‘¨æœ«)ã€‚")

    model_input_cols = ['open', 'high', 'low', 'close', 'volume', 'timestamps']
    df_for_model = df_cleaned[model_input_cols]

    logger.info(f"å‡†å¤‡è°ƒç”¨é¢„æµ‹å™¨... å†å²æ•°æ® {len(df_for_model)} æ¡ (Volume å·² Log å˜æ¢)ã€‚")

    # 5. è°ƒç”¨ predict
    fig: Optional[go.Figure] = None  # <--- åˆå§‹åŒ– fig
    try:
        with torch.inference_mode():
            pred_df = predictor.predict(
                df=df_for_model,
                x_timestamp=x_timestamp,
                y_timestamp=y_timestamp,
                pred_len=pred_len,
                T=T,
                top_p=top_p,
                sample_count=sample_count
            )

        if 'volume' in pred_df.columns:
            pred_df['volume'] = np.expm1(pred_df['volume'])
            logger.info("  - (å…³é”®) å·²åè½¬é¢„æµ‹ç»“æœ 'volume' çš„ log1p å˜æ¢ (expm1)")

        logger.info("é¢„æµ‹æˆåŠŸã€‚")
        print("\n--- é¢„æµ‹ç»“æœ (Head) ---")
        print(pred_df[["open", "high", "low", "close", "volume"]].head())
        print("------------------------\n")

        # 6. ç»˜å›¾
        logger.info("ç”Ÿæˆé¢„æµ‹å¯¹æ¯”å›¾...")
        fig = build_prediction_figure(df_cleaned, pred_df)  # <--- fig åœ¨è¿™é‡Œè¢«èµ‹å€¼

        #  ä¿å­˜å›¾è¡¨åˆ°æœ¬åœ°æ–‡ä»¶ ---
        try:
            global output_dir
            safe_end_date_str = end_ts.strftime("%Y%m%d")
            filename = f"{symbol}_{safe_end_date_str}_pred_{pred_len}d.html"
            save_path = os.path.join(output_dir, filename)

            fig.write_html(save_path)

            logger.info(f"é¢„æµ‹å›¾è¡¨å·²æˆåŠŸä¿å­˜åˆ°: {save_path}")
            print(f"f é¢„æµ‹å›¾è¡¨å·²æˆåŠŸä¿å­˜åˆ°: {save_path}")

        except Exception as e:
            logger.error(f"ä¿å­˜ HTML å›¾è¡¨å¤±è´¥: {e}")
            print(f"âŒ ä¿å­˜ HTML å›¾è¡¨å¤±è´¥: {e}")

        logger.info(f"--- [END] é¢„æµ‹å®Œæˆ {symbol} ---")

        # <--- [å…³é”®ä¿®æ”¹] ---
        return df_cleaned, pred_df, fig  # <--- è¿”å› fig å¯¹è±¡

    except Exception as e:
        logger.exception(f"é¢„æµ‹æˆ–ç»˜å›¾è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        # <--- [å…³é”®ä¿®æ”¹] ---
        # è¿”å› df_cleanedï¼ˆå¦‚æœå·²ç”Ÿæˆï¼‰ï¼Œä½† pred_df å’Œ fig ä¸º None
        return df_cleaned, None, fig

    # -----------------------------------------------------------------


# 7. æ‰§è¡Œå…¥å£ (å·²ä¿®æ”¹)
# -----------------------------------------------------------------
if __name__ == "__main__":

    # --- ç¤ºä¾‹è°ƒç”¨ 1 (ä½¿ç”¨ç‰¹å®šæ—¥æœŸï¼Œè¦†ç›–é»˜è®¤å€¼) ---
    logger.info("--- [ç¤ºä¾‹ 1] è¿è¡Œç‰¹å®šæ—¥æœŸé¢„æµ‹ (601519) ---")

    # [ä¿®æ”¹] æ¥æ”¶ fig_1
    hist_df_1, pred_df_1, fig_1 = kronos_predict(
        symbol="601519",  # å¤§æ™ºæ…§
        start_date_str="2020-01-01",
        end_date_str="2025-05-15",
        pred_len=100  # è¦†ç›–é»˜è®¤çš„ 90
    )

    # æ£€æŸ¥é¢„æµ‹æ˜¯å¦æˆåŠŸ
    if pred_df_1 is not None:
        logger.info("ç¤ºä¾‹ 1 é¢„æµ‹æˆåŠŸã€‚")
        result = analyze_prediction_df(pred_df_1, freq_per_year=252, bootstrap_iters=2000)
        print("\n--- é¢„æµ‹åˆ†ææŠ¥å‘Š (ç¤ºä¾‹ 1) ---")
        print(result['text_report'])
        print("---------------------------------\n")

        # å¦‚æœæ‚¨åœ¨ Jupyter Notebook ä¸­è¿è¡Œï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡Šä¸‹é¢è¿™è¡Œæ¥æ˜¾ç¤ºå›¾è¡¨
        fig_1.show()
    else:
        logger.error("ç¤ºä¾‹ 1 é¢„æµ‹å¤±è´¥ã€‚")

    logger.info("--- [ç¤ºä¾‹ 1] å®Œæˆ ---\n")

    # --- ç¤ºä¾‹è°ƒç”¨ 2 (ä½¿ç”¨é»˜è®¤æ—¥æœŸï¼šè¿‘3å¹´ -> é¢„æµ‹ 90 å¤©) ---
    logger.info("--- [ç¤ºä¾‹ 2] è¿è¡Œé»˜è®¤æ—¥æœŸé¢„æµ‹ (601519) ---")

    # [ä¿®æ”¹] æ¥æ”¶ fig_2
    hist_df_2, pred_df_2, fig_2 = kronos_predict(
        symbol="601519"  # å¤§æ™ºæ…§
        # start_date_str=None, (ä½¿ç”¨é»˜è®¤å€¼: 3å¹´å‰)
        # end_date_str=None, (ä½¿ç”¨é»˜è®¤å€¼: ä»Šå¤©)
        # pred_len=90 (ä½¿ç”¨é»˜è®¤å€¼)
    )

    if pred_df_2 is not None:
        logger.info("ç¤ºä¾‹ 2 é¢„æµ‹æˆåŠŸã€‚")
        result_2 = analyze_prediction_df(pred_df_2, freq_per_year=252, bootstrap_iters=2000)
        print("\n--- é¢„æµ‹åˆ†ææŠ¥å‘Š (ç¤ºä¾‹ 2) ---")
        print(result_2['text_report'])
        print("---------------------------------\n")
        fig_2.show()
    else:
        logger.error("ç¤ºä¾‹ 2 é¢„æµ‹å¤±è´¥ã€‚")

    logger.info("--- [ç¤ºä¾‹ 2] å®Œæˆ ---\n")