import os
import json
import time
import requests
import pandas as pd
from datetime import datetime

# ==============================================================================
# 1. åº•å±‚è·å–å‡½æ•°ï¼šç›´æ¥è¯·æ±‚ä¸œæ–¹è´¢å¯Œæ¥å£ (æ›¿ä»£ akshare)
# ==============================================================================

def fetch_eastmoney_news(symbol: str) -> pd.DataFrame:
    """
    åº•å±‚å‡½æ•°ï¼šç›´æ¥ä»ä¸œæ–¹è´¢å¯Œæ¥å£è·å–ä¸ªè‚¡æ–°é—»
    åŒ…å«ï¼šåŠ¨æ€å›è°ƒåã€User-Agentä¼ªè£…ã€JSON/JSONP åŒé‡è§£ææœºåˆ¶
    """
    # åŠ¨æ€ç”Ÿæˆ callback å‚æ•°
    callback_name = f"callback_{int(time.time() * 1000)}"
    url = "http://search-api-web.eastmoney.com/search/jsonp"
    
    # æ„é€ è¯·æ±‚å‚æ•°
    params = {
        "cb": callback_name,
        "param": '{"uid":"",'
        + f'"keyword":"{symbol}"'
        + ',"type":["cmsArticle"],"client":"web","clientType":"web","clientVersion":"curr",'
        + '"param":{"cmsArticle":{"searchScope":"default","sort":"default","pageIndex":1,'
        + '"pageSize":100,"preTag":"<em>","postTag":"</em>"}}}',
    }
    
    # æ¨¡æ‹Ÿæµè§ˆå™¨ Header
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        # å‘é€è¯·æ±‚
        r = requests.get(url, params=params, headers=headers, timeout=10)
        r.raise_for_status()
        data_text = r.text
        
        data_json = None
        
        # --- è§£æé€»è¾‘ ---
        # å°è¯•æ–¹å¼ A: çº¯ JSON
        try:
            data_json = r.json()
        except json.JSONDecodeError:
            # å°è¯•æ–¹å¼ B: JSONP (å‰¥ç¦» callback(...))
            start_index = data_text.find('(')
            end_index = data_text.rfind(')')
            if start_index != -1 and end_index != -1 and end_index > start_index:
                json_string = data_text[start_index + 1 : end_index]
                try:
                    data_json = json.loads(json_string)
                except:
                    pass
        
        if not data_json:
            print(f"é”™è¯¯: æ— æ³•è§£æä¸œæ–¹è´¢å¯Œè¿”å›çš„æ•°æ®æ ¼å¼ - {symbol}")
            return pd.DataFrame()

        # --- æ•°æ®æå–ä¸æ¸…æ´— ---
        if "result" not in data_json or "cmsArticle" not in data_json["result"]:
            return pd.DataFrame()
            
        temp_df = pd.DataFrame(data_json["result"]["cmsArticle"])
        if temp_df.empty:
            return pd.DataFrame()

        # æ„é€ æ ‡å‡†å­—æ®µ
        temp_df["url"] = "http://finance.eastmoney.com/a/" + temp_df["code"] + ".html"
        temp_df.rename(columns={
            "date": "å‘å¸ƒæ—¶é—´",
            "mediaName": "æ–‡ç« æ¥æº",
            "title": "æ–°é—»æ ‡é¢˜",
            "content": "æ–°é—»å†…å®¹",
            "url": "æ–°é—»é“¾æ¥"
        }, inplace=True)
        
        temp_df["å…³é”®è¯"] = symbol
        
        # ç¡®ä¿æ‰€éœ€åˆ—å­˜åœ¨
        required_cols = ["å…³é”®è¯", "æ–°é—»æ ‡é¢˜", "æ–°é—»å†…å®¹", "å‘å¸ƒæ—¶é—´", "æ–‡ç« æ¥æº", "æ–°é—»é“¾æ¥"]
        for col in required_cols:
            if col not in temp_df.columns:
                temp_df[col] = ""

        final_df = temp_df[required_cols].copy()

        # æ¸…æ´— HTML æ ‡ç­¾ (<em>...</em>)
        for col in ["æ–°é—»æ ‡é¢˜", "æ–°é—»å†…å®¹"]:
            final_df[col] = (
                final_df[col]
                .astype(str)
                .str.replace(r"\(<em>", "", regex=True)
                .str.replace(r"</em>\)", "", regex=True)
                .str.replace(r"<em>", "", regex=True)
                .str.replace(r"</em>", "", regex=True)
            )
            
        final_df["æ–°é—»å†…å®¹"] = final_df["æ–°é—»å†…å®¹"].str.replace(r"\u3000", "", regex=True).str.replace(r"\r\n", " ", regex=True)
        
        return final_df

    except Exception as e:
        print(f"è·å– {symbol} æ–°é—»æ—¶å‘ç”Ÿç½‘ç»œæˆ–è§£æé”™è¯¯: {e}")
        return pd.DataFrame()


# ==============================================================================
# 2. ä¸­é—´å±‚ï¼šå¤„ç†æ•°æ®æ ¼å¼ (æ›¿ä»£åŸæœ‰çš„ akshare è°ƒç”¨é€»è¾‘)
# ==============================================================================

def get_stock_news_via_akshare(symbol: str, max_news: int = 100) -> list:
    """
    è·å–è‚¡ç¥¨æ–°é—»çš„ä¸­é—´å±‚å¤„ç†å‡½æ•°ã€‚
    æ­¤å¤„ä¸å†ä¾èµ– akshare åº“ï¼Œè€Œæ˜¯è°ƒç”¨è‡ªå®šä¹‰çš„ fetch_eastmoney_newsã€‚
    """
    try:
        # [ä¿®æ”¹ç‚¹] è°ƒç”¨è‡ªå®šä¹‰å‡½æ•°ï¼Œè€Œä¸æ˜¯ ak.stock_news_em
        news_df = fetch_eastmoney_news(symbol)

        if news_df is None or len(news_df) == 0:
            print(f"æœªè·å–åˆ° {symbol} çš„æ–°é—»æ•°æ®")
            return []

        # å®é™…å¯è·å–çš„æ–°é—»æ•°é‡
        available_news_count = len(news_df)
        fetch_count = min(available_news_count, int(max_news * 1.5)) # å¤šå–ä¸€ç‚¹ä»¥é˜²è¿‡æ»¤

        news_list = []
        
        # éå† DataFrame (é€»è¾‘ä¸åŸæ¥ä¿æŒä¸€è‡´ï¼Œä¿è¯å…¼å®¹æ€§)
        for _, row in news_df.head(fetch_count).iterrows():
            try:
                # è·å–å¹¶å¤„ç†å†…å®¹
                content = row["æ–°é—»å†…å®¹"] if not pd.isna(row["æ–°é—»å†…å®¹"]) else ""
                if not content:
                    content = row["æ–°é—»æ ‡é¢˜"] # é™çº§ç­–ç•¥

                content = content.strip()
                if len(content) < 10:  # è¿‡æ»¤è¿‡çŸ­å†…å®¹
                    continue

                keyword = row["å…³é”®è¯"] if not pd.isna(row["å…³é”®è¯"]) else ""

                news_item = {
                    "title": row["æ–°é—»æ ‡é¢˜"].strip(),
                    "content": content,
                    "publish_time": row["å‘å¸ƒæ—¶é—´"],
                    "source": row["æ–‡ç« æ¥æº"].strip(),
                    "url": row["æ–°é—»é“¾æ¥"].strip(),
                    "keyword": keyword.strip()
                }
                news_list.append(news_item)

            except Exception as e:
                print(f"å¤„ç†å•æ¡æ–°é—»æ•°æ®å‡ºé”™: {e}")
                continue

        # æŒ‰æ—¶é—´é™åº
        news_list.sort(key=lambda x: x["publish_time"], reverse=True)
        
        return news_list[:max_news]

    except Exception as e:
        print(f"è·å–æ–°é—»æµç¨‹å‡ºé”™: {e}")
        return []


# ==============================================================================
# 3. ä¸Šå±‚æ¥å£ï¼šç¼“å­˜ä¸æ–‡ä»¶ç®¡ç† (ä¿æŒåŸæœ‰é€»è¾‘ä¸å˜)
# ==============================================================================

def get_stock_news(symbol: str, max_news: int = 10, date: str = None) -> list:
    """
    è·å–å¹¶å¤„ç†ä¸ªè‚¡æ–°é—» (åŒ…å«æœ¬åœ°ç¼“å­˜æœºåˆ¶)
    """
    # é™åˆ¶æœ€å¤§æ–°é—»æ¡æ•°
    max_news = min(max_news, 100)

    # è·å–å½“å‰æ—¥æœŸæˆ–ä½¿ç”¨æŒ‡å®šæ—¥æœŸ
    cache_date = date if date else datetime.now().strftime("%Y-%m-%d")

    # æ„å»ºæ–°é—»æ–‡ä»¶è·¯å¾„
    news_dir = os.path.join("data", "stock_news")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    try:
        os.makedirs(news_dir, exist_ok=True)
    except Exception as e:
        print(f"åˆ›å»ºç›®å½•å¤±è´¥: {e}")
        return []

    # ç¼“å­˜æ–‡ä»¶ååŒ…å«æ—¥æœŸä¿¡æ¯
    news_file = os.path.join(news_dir, f"{symbol}_news_{cache_date}.json")

    # --- æ£€æŸ¥ç¼“å­˜ ---
    cached_news = []
    cache_valid = False

    if os.path.exists(news_file):
        try:
            file_mtime = os.path.getmtime(news_file)
            # ç¼“å­˜æœ‰æ•ˆæœŸç­–ç•¥
            if date:  # æŒ‡å®šå†å²æ—¥æœŸï¼Œç¼“å­˜æ°¸ä¹…æœ‰æ•ˆ
                cache_valid = True
            else:     # å½“å¤©æ•°æ®ï¼Œè¦æ±‚æ–‡ä»¶å¿…é¡»æ˜¯ä»Šå¤©ç”Ÿæˆçš„
                cache_date_obj = datetime.fromtimestamp(file_mtime).date()
                today = datetime.now().date()
                cache_valid = cache_date_obj == today

            if cache_valid:
                with open(news_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    cached_news = data.get("news", [])

                    if len(cached_news) >= max_news:
                        # print(f"ä½¿ç”¨ç¼“å­˜æ•°æ®: {len(cached_news)}æ¡")
                        return cached_news[:max_news]
            else:
                print(f"ç¼“å­˜å·²è¿‡æœŸï¼Œé‡æ–°è·å–...")

        except Exception as e:
            print(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
            cached_news = []

    # --- è·å–æ–°æ•°æ® ---
    need_more_news = max_news - len(cached_news)
    fetch_count = max(need_more_news, max_news)

    # [è°ƒç”¨ä¿®æ”¹åçš„ä¸­é—´å±‚å‡½æ•°]
    new_news_list = get_stock_news_via_akshare(symbol, fetch_count)

    # --- åˆå¹¶ä¸å»é‡ ---
    if cached_news and new_news_list:
        existing_titles = {news['title'] for news in cached_news}
        unique_new_news = [n for n in new_news_list if n['title'] not in existing_titles]
        combined_news = cached_news + unique_new_news
    else:
        combined_news = new_news_list or cached_news

    # æ’åº
    try:
        combined_news.sort(key=lambda x: x.get("publish_time", ""), reverse=True)
    except:
        pass

    final_news_list = combined_news[:max_news]

    # --- ä¿å­˜ç¼“å­˜ ---
    if new_news_list or not cache_valid:
        try:
            save_data = {
                "date": cache_date,
                "method": "custom_eastmoney_api", # æ ‡è®°ä¸€ä¸‹æ–¹æ³•å˜äº†
                "news": combined_news,
                "last_updated": datetime.now().isoformat()
            }
            with open(news_file, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜ç¼“å­˜å‡ºé”™: {e}")

    return final_news_list

if __name__ == "__main__":
    TEST_SYMBOL = "600519"  # è´µå·èŒ…å°
    TEST_MAX_NEWS = 10
    TEST_DATE = None  # None è¡¨ç¤ºä½¿ç”¨å½“å‰æ—¥æœŸ

    print("=" * 40)
    print("      ğŸš€ å¼€å§‹è¿è¡Œè‚¡ç¥¨æ–°é—»ç¼“å­˜ç³»ç»Ÿæµ‹è¯• ğŸš€")
    print("=" * 40)
    
    # æµ‹è¯•è·å–æ–°é—»
    print(f"\næµ‹è¯•è‚¡ç¥¨: {TEST_SYMBOL}")
    print(f"æœ€å¤§æ–°é—»æ¡æ•°: {TEST_MAX_NEWS}")
    print(f"æ—¥æœŸ: {TEST_DATE or 'å½“å‰æ—¥æœŸ'}")
    
    news = get_stock_news(TEST_SYMBOL, TEST_MAX_NEWS, TEST_DATE)
    
    if news:
        print(f"\næˆåŠŸè·å– {len(news)} æ¡æ–°é—»")
        for item in news[:3]:  # å±•ç¤ºå‰ 3 æ¡
            print(f"\næ ‡é¢˜: {item['title']}")
            print(f"æ¥æº: {item['source']}")
            print(f"æ—¶é—´: {item['publish_time']}")